{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Generate The Sentence Embeddings and Do Tests Here\n",
        "Do this only once"
      ],
      "metadata": {
        "id": "KnE1KgvJ-kOd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mAo81O_YgIfm",
        "outputId": "00def6d2-6fa5-4f86-ead3-c7646843f8aa"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/ATAI/project\n",
        "! mkdir processed\n",
        "%cd processed"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OZREKc1UB1f2",
        "outputId": "07e78da6-c6bd-4ef0-e1d5-1906d2826b29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/ATAI/project/processed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ab5_R_R7UWHA"
      },
      "outputs": [],
      "source": [
        "!pip install rdflib\n",
        "!pip install networkx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "qsEynPSV4FKf",
        "outputId": "b2609d18-eada-46ac-a489-214e931ef072"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "        <script type=\"text/javascript\">\n",
              "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
              "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
              "        if (typeof require !== 'undefined') {\n",
              "        require.undef(\"plotly\");\n",
              "        requirejs.config({\n",
              "            paths: {\n",
              "                'plotly': ['https://cdn.plot.ly/plotly-2.8.3.min']\n",
              "            }\n",
              "        });\n",
              "        require(['plotly'], function(Plotly) {\n",
              "            window._Plotly = Plotly;\n",
              "        });\n",
              "        }\n",
              "        </script>\n",
              "        "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from rdflib.namespace import Namespace, RDF, RDFS, XSD\n",
        "from rdflib.term import URIRef, Literal\n",
        "import csv\n",
        "import json\n",
        "import networkx as nx\n",
        "import pandas as pd\n",
        "import rdflib\n",
        "from collections import defaultdict, Counter\n",
        "import locale\n",
        "_ = locale.setlocale(locale.LC_ALL, '')\n",
        "from _plotly_future_ import v4_subplots\n",
        "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
        "import plotly.graph_objs as go\n",
        "init_notebook_mode(connected=True)\n",
        "import numpy as np\n",
        "import os, random\n",
        "from tqdm import tqdm\n",
        "from joblib import dump, load\n",
        "from sklearn.metrics import pairwise_distances\n",
        "from sklearn.feature_extraction.text import strip_accents_ascii\n",
        "import pickle\n",
        "\n",
        "from string import ascii_letters\n",
        "from urllib.parse import urlparse\n",
        "from difflib import SequenceMatcher \n",
        "\n",
        "only_letters = set(ascii_letters + ' ')\n",
        "\n",
        "\n",
        "def normalize_text(text):\n",
        "  t = text.replace(\" â€“ \", \" - \")\n",
        "  t = strip_accents_ascii(t.lower())\n",
        "  t = t.replace('?', ' ')\n",
        "  t = t.replace('.', ' ')\n",
        "  t = t.replace(',', ' ')\n",
        "  t = t.replace('\\n', ' ')\n",
        "  t = ' '.join(t.split())\n",
        "  return ' ' + t + ' '"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prefixes used in the graph\n",
        "WD = Namespace('http://www.wikidata.org/entity/')\n",
        "WDT = Namespace('http://www.wikidata.org/prop/direct/')\n",
        "SCHEMA = Namespace('http://schema.org/')\n",
        "DDIS = Namespace('http://ddis.ch/atai/')\n",
        "\n",
        "%cd /content/drive/MyDrive/ATAI/project\n",
        "\n",
        "# some very useful relations\n",
        "label_pred = rdflib.term.URIRef('http://www.w3.org/2000/01/rdf-schema#label')\n",
        "imdb_id = WDT.P345\n",
        "image = WDT.P18\n",
        "genre_pred = WDT.P136\n",
        "type_pred = WDT.P31\n",
        "movie_obj = WD.Q11424"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "61_uhgN12IXJ",
        "outputId": "cfc341e0-630a-4083-a49e-6e2a55d61db4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/ATAI/project\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# save the graph in pickle - so faster to load\n",
        "%cd /content/drive/MyDrive/ATAI/project\n",
        "graph = rdflib.Graph()\n",
        "graph.parse('./14_graph.nt', format='turtle')"
      ],
      "metadata": {
        "id": "ToVIP06J6B2d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "with open('graph.pkl', 'wb') as file: \n",
        "    # A new file will be created\n",
        "    pickle.dump(graph, file)"
      ],
      "metadata": {
        "id": "eSnP20qL9LeQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('graph.pkl', 'rb') as file: \n",
        "    # Call load method to deserialze\n",
        "    graph = pickle.load(file)"
      ],
      "metadata": {
        "id": "ExEUqX5GQu5U"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dF0ouXGRZiob"
      },
      "outputs": [],
      "source": [
        "entities = set(graph.subjects()) | {s for s in graph.objects() if isinstance(s, URIRef)}\n",
        "predicates = set(graph.predicates())\n",
        "literals = {s for s in graph.objects() if isinstance(s, Literal)}\n",
        "with_type = set(graph.subjects(WDT['P31'], None))\n",
        "with_super = set(graph.subjects(WDT['P279'], None))\n",
        "types = set(graph.objects(None, WDT['P31']))\n",
        "supers = set(graph.objects(None, WDT['P279']))\n",
        "with_label = set(graph.subjects(RDFS.label, None))\n",
        "\n",
        "n_ents = len(entities)\n",
        "n_rels = len(predicates)\n",
        "n_lits = len(literals)\n",
        "t_tot = len(graph)\n",
        "t_ent = len([1 for s,p,o in graph.triples((None, None, None)) if isinstance(o, URIRef)])\n",
        "t_lit = t_tot - t_ent\n",
        "n_notype = len(entities - with_type - with_super)\n",
        "n_notype_flt = len(entities - with_type - with_super - types - supers)\n",
        "\n",
        "pd.DataFrame([\n",
        "    ('number of entities', f'{n_ents:n}'),\n",
        "    ('number of literals', f'{n_lits:n}'),\n",
        "    ('number of predicates', f'{n_rels:n}'),\n",
        "    ('number of triples', f'{t_tot:n}'),\n",
        "    ('number of ent-ent triples', f'{t_ent:n}'),\n",
        "    ('number of ent-lit triples', f'{t_lit:n}'),\n",
        "    ('number of entities w/o label', f'{len(entities - with_label):n}'),\n",
        "    ('number of predicates w/o label', f'{len(predicates - with_label):n}'),\n",
        "    ('number of entities w/o type', f'{n_notype:n}'),\n",
        "    ('number of instances w/o type', f'{n_notype_flt:n}'),\n",
        "    ])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# process predicates and entities data\n",
        "\n",
        "%cd processed\n",
        "entities_no_label = entities - with_label\n",
        "pred_without_label = predicates - with_label\n",
        "\n",
        "predicates_dict = {}\n",
        "# col1: predicate URL, col2: label\n",
        "\n",
        "for id, pred in enumerate(predicates-pred_without_label):\n",
        "  generator = graph.objects(pred, RDFS.label)\n",
        "  name = normalize_text(str(list(generator)[0]))\n",
        "  if name not in predicates_dict:\n",
        "    predicates_dict[name] = [pred] \n",
        "  else:\n",
        "    predicates_dict[name].append(pred)\n",
        "\n",
        "predicates_dict['tag'] = rdflib.term.URIRef('http://ddis.ch/atai/tag')\n",
        "predicates_dict['rating'] = rdflib.term.URIRef('http://ddis.ch/atai/rating')\n",
        "\n",
        "entities_dict = {}\n",
        "# col1: predicate URL, col2: label\n",
        "\n",
        "# ignore no label ones for now (they actually do not have names)\n",
        "for id, ent in tqdm(enumerate(entities-entities_no_label)):\n",
        "  generator = graph.objects(ent, RDFS.label)\n",
        "  name = normalize_text(str(list(generator)[0]))\n",
        "  if name not in entities_dict:\n",
        "    entities_dict[name] = [ent]\n",
        "  else:\n",
        "    entities_dict[name].append(ent)"
      ],
      "metadata": {
        "id": "rmwK6XSa-VpD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dump(predicates_dict, 'predicates_dict.joblib') \n",
        "dump(entities_dict, 'entities_dict.joblib')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NxwhCM6zxVIW",
        "outputId": "9af45025-f532-4d61-d55f-4c4f0f254483"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['entities_dict.joblib']"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install -U sentence-transformers\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# sentence transformer model\n",
        "sentence_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')"
      ],
      "metadata": {
        "id": "aP61-O_6V1Y4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get sentence embeddings for all predicates and entities and save them\n",
        "\n",
        "import numpy as np\n",
        "from joblib import dump, load\n",
        "\n",
        "%cd /content/drive/MyDrive/ATAI/project/processed\n",
        "predicates_dict = load('predicates_dict.joblib')\n",
        "entities_dict = load('entities_dict.joblib')\n",
        "\n",
        "sentences = list(entities_dict.keys())\n",
        "embeddings = sentence_model.encode(sentences)\n",
        "embeddings = np.array(embeddings)\n",
        "np.save('entities_embeddings.npy', embeddings)\n",
        "\n",
        "sentences = list(predicates_dict.keys())\n",
        "embeddings = sentence_model.encode(sentences)\n",
        "embeddings = np.array(embeddings)\n",
        "np.save('predicates_embeddings.npy', embeddings)"
      ],
      "metadata": {
        "id": "0Pm4w3AS-axN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4aa1f68-2ec9-44aa-c455-baac802a1978"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/ATAI/project/processed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# save images_dict as pickle too\n",
        "with open('/content/drive/MyDrive/ATAI/project/movienet/images.json', 'r') as f:\n",
        "  images_dict = json.load(f)\n",
        "\n",
        "import pickle\n",
        "with open('images.pkl', 'wb') as file: \n",
        "    # A new file will be created\n",
        "    pickle.dump(images_dict, file)"
      ],
      "metadata": {
        "id": "uZRXmBS1SDdA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LOAD EVERYTHING NOW"
      ],
      "metadata": {
        "id": "hW36Aywh2Bvm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('graph.pkl', 'rb') as file: \n",
        "  # Call load method to deserialze\n",
        "  graph = pickle.load(file)\n",
        "\n",
        "# load the dictionaries\n",
        "%cd /content/drive/MyDrive/ATAI/project/processed\n",
        "predicates_dict = load('predicates_dict.joblib')\n",
        "entities_dict = load('entities_dict.joblib')\n",
        "\n",
        "predicates_names = list(predicates_dict.keys())\n",
        "entities_names = list(entities_dict.keys())\n",
        "\n",
        "ent2lbl = {}\n",
        "for key, value in list(entities_dict.items()):\n",
        "  for val in value:\n",
        "    ent2lbl[val] = key\n",
        "\n",
        "pred2lbl = {}\n",
        "for key, value in list(predicates_dict.items()):\n",
        "  for val in value:\n",
        "    pred2lbl[val] = key\n",
        "\n",
        "# for graph embeddings:\n",
        "%cd /content/drive/MyDrive/ATAI/project\n",
        "with open('./ddis-graph-embeddings/entity_ids.del', 'r') as ifile:\n",
        "    ent2id = {rdflib.term.URIRef(ent): int(idx) for idx, ent in csv.reader(ifile, delimiter='\\t')}\n",
        "    id2ent = {v: k for k, v in ent2id.items()}\n",
        "with open('./ddis-graph-embeddings/relation_ids.del', 'r') as ifile:\n",
        "    pred2id = {rdflib.term.URIRef(rel): int(idx) for idx, rel in csv.reader(ifile, delimiter='\\t')}\n",
        "    id2pred = {v: k for k, v in pred2id.items()}\n",
        "\n",
        "# load the sentence embeddings\n",
        "%cd /content/drive/MyDrive/ATAI/project\n",
        "entities_graph_embeddings = np.load('./ddis-graph-embeddings/entity_embeds.npy')\n",
        "predicates_graph_embeddings = np.load('./ddis-graph-embeddings/relation_embeds.npy')\n",
        "\n",
        "%cd processed\n",
        "entities_embeddings = np.load('entities_embeddings.npy')\n",
        "predicates_embeddings = np.load('predicates_embeddings.npy')\n",
        "\n",
        "with open('images.pkl', 'rb') as file: \n",
        "  # Call load method to deserialze\n",
        "  images_dict = pickle.load(file)\n",
        "\n",
        "greet_emb = sentence_model.encode('Hello, how are you?')"
      ],
      "metadata": {
        "id": "Fvd7_jdS-0Wv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# No need for this now!\n",
        "\n",
        "! pip install transformers\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
        "from transformers import pipeline\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-base-NER\")\n",
        "model = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-base-NER\")\n",
        "\n",
        "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "K9RbLlKQ1C-1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# backup option\n",
        "import requests\n",
        "\n",
        "API_URL = \"https://api-inference.huggingface.co/models/satvikag/chatbot\"\n",
        "headers = {\"Authorization\": f\"Bearer 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\"}\n",
        "\n",
        "def query(q):    \n",
        "    response = requests.post(API_URL, headers=headers, json={\"inputs\": {\"text\": q}}).json()\n",
        "    if 'generated_text' in response:\n",
        "        return response['generated_text']\n",
        "    else:\n",
        "        return 'Sorry no reply'"
      ],
      "metadata": {
        "id": "ihXY9BUOOQx8"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing the Functions"
      ],
      "metadata": {
        "id": "HnR_UNnaDZqK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_questions = ['Who is the director of Good Will Hunting?', \n",
        "                  'Who directed The Bridge on the River Kwai?',\n",
        "                  'Who is the director of Star Wars: Episode VI - Return of the Jedi?',\n",
        "                  'Who is the screenwriter of The Masked Gang: Cyprus?',\n",
        "                  'What is the MPAA film rating of Weathering with You?',\n",
        "                  'What is the genre of Good Neighbors?',\n",
        "                  'Show me a picture of Halle Berry.',\n",
        "                  'What does Julia Roberts look like?',\n",
        "                  'Let me know what Sandra Bullock looks like.',\n",
        "                  'Recommend movies similar to Hamlet and Othello.',\n",
        "                  'Given that I like The Lion King, Pocahontas, and The Beauty and the Beast, can you recommend some movies?',\n",
        "                  'Recommend movies like Nightmare on Elm Street, Friday the 13th, and Halloween.',\n",
        "                  'What is the box office of The Princess and the Frog?',\n",
        "                  'Can you tell me the publication date of Tom Meets Zizou?',\n",
        "                  'Who is the executive producer of X-Men: First Class?',\n",
        "                  'Who is Top Gun: Maverick\\'s screenwriter?']\n"
      ],
      "metadata": {
        "id": "JhRn36WC12g-"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# the same functions as in the query/__init__.py for testing\n",
        "\n",
        "templates = {'KB':['The answer found in the graph is: ', 'According to the knowledge graph: ', 'I think the answer is '], \n",
        "             'emb':['The answer found using embeddings is: ', 'The answer suggested by embeddings: ',\n",
        "                    'Some other answers found using the embeddings: '],\n",
        "             'no':['Sorry, I could not find the information you are loking for. Can you paraphrase your question?', \n",
        "                   'The information could not be found.', 'Sorry, can you rephrase your question?'],\n",
        "             'suggest':['Some %s similar to those are: '], # !!!!!!!!!!!!!!!!\n",
        "             'image':['Here is a %s image from the movie %s (%s):\\n%s', \n",
        "                      'Here is an image of %s from the movie %s (%s):\\n%s',\n",
        "                      'Here is an image of %s:\\n%s'],\n",
        "             'greet':['Hey!', 'Hello', 'Hi :)'],\n",
        "             'recommend': [('You can check out the movies ', ' with the genres ', ' which I have found using the graph embeddings.\\n'),\n",
        "                           'You can check out the movie %s found using the %s. Its genre is %s.',\n",
        "                           'You might be interested in this movie: %s that I found in the %s, with the genre %s.',\n",
        "                           'Here is the imdb page of the movie: imdb:\\s',\n",
        "                            'See the imdb page here: imdb:%s.',\n",
        "                            '(imdb:%s)']} # or use anoher nlp model?\n",
        "sentence_types = ['full', 'only_obj']\n",
        "\n",
        "# ************************************************ANSWER*************************************************\n",
        "def answer(q, k=3):\n",
        "  triples, embeddings_triples = extract(q)\n",
        "  if triples == None:\n",
        "      return query(q)\n",
        "  if type(triples) == str:\n",
        "    return triples\n",
        "  ans = ''\n",
        "  use_other = False\n",
        "  if len(triples) > 0:\n",
        "    ans += templates['KB'][random.randint(0,1)] + construct_answer(triples, type=sentence_types[random.randint(0,1)]) + '.\\n'\n",
        "    use_other = True\n",
        "    for triple in triples:\n",
        "      for emb_triple in embeddings_triples:\n",
        "        if triple[:2] == emb_triple[:2]:\n",
        "          emb_triple[2] = list(set(emb_triple[2]) - set(triple[2]))\n",
        "\n",
        "  if len(embeddings_triples) > 0:\n",
        "    if use_other:\n",
        "      ans += templates['emb'][2] + construct_answer(embeddings_triples, type=sentence_types[1]) + '.\\n'\n",
        "    else:\n",
        "      ans += templates['emb'][random.randint(0,1)] + construct_answer(embeddings_triples, type=sentence_types[random.randint(0,1)]) + '.\\n'\n",
        "  if len(triples) <= 0 and len(embeddings_triples) <= 0:\n",
        "    # use a fun model!\n",
        "    return query(q)\n",
        "    # ans = templates['no'][random.randint(0,2)]\n",
        "  return ans\n",
        "\n",
        "# type can be full, only_obj ...\n",
        "def construct_answer(triples, type='full'):\n",
        "  def mult_obj(objects):\n",
        "    ans = ''\n",
        "    for obj in objects[:-1]:\n",
        "      ans += ' %s,'%obj\n",
        "    ans = ans[:-1]\n",
        "    ans += ' and %s'%objects[-1]\n",
        "    return ans\n",
        "\n",
        "  ans = ''\n",
        "  if type=='full':\n",
        "    for triple in triples:\n",
        "      objects = triple[2]\n",
        "      mult = ''\n",
        "      if len(objects) > 1:\n",
        "        mult = 's'\n",
        "      ans = 'The %s%s of %s '%(triple[1], mult, triple[0])\n",
        "      if len(objects) == 1:\n",
        "        ans += 'is %s'%objects[0]\n",
        "      else:\n",
        "        ans += 'are'\n",
        "        ans += mult_obj(objects)\n",
        "    return ans\n",
        "  elif type=='only_obj':\n",
        "    triple = triples[0]\n",
        "    objects = triple[2]\n",
        "    mult = ''\n",
        "    if len(objects) == 1:\n",
        "      return objects[0]\n",
        "    else:\n",
        "      return mult_obj(objects)\n",
        "  else:\n",
        "    triple = triples[0]\n",
        "    objects = triple[2]\n",
        "    mult = ''\n",
        "    if len(objects) == 1:\n",
        "      return objects[0]\n",
        "    else:\n",
        "      return mult_obj(objects) \n",
        "\n",
        "def get_imdb_id(ent):\n",
        "  for s, p, o in graph.triples((ent, imdb_id, None)):\n",
        "    return str(o)\n",
        "  return None\n",
        "\n",
        "# all these should be strings\n",
        "def get_image_from_imdb_id(movie=None, im_type=None, cast=None, only_person=True):\n",
        "  ims = images_dict.copy()\n",
        "  if movie != None:\n",
        "    ims = [d for d in ims if movie in d['movie']]\n",
        "  if im_type != None:\n",
        "    ims = [d for d in ims if d['type'] == im_type]\n",
        "  if cast != None:\n",
        "    if only_person:\n",
        "      ims = [d for d in ims if cast in d['cast'] and len(d['cast']) == 1]\n",
        "    else:\n",
        "      ims = [d for d in ims if cast in d['cast'] and len(d['cast']) > 1]\n",
        "  return ims\n",
        "\n",
        "def answer_image(ent):\n",
        "  id = get_imdb_id(ent)\n",
        "  if id != None and id.startswith('tt'):  # we are looking for a movie img\n",
        "    ims = get_image_from_imdb_id(movie=id)\n",
        "    if len(ims) <= 0:\n",
        "      return None, None\n",
        "    im = ims[random.randint(0,len(ims)-1)]\n",
        "    im_type = im['type']\n",
        "    return templates['image'][0]%(im_type, ent2lbl[ent].title(), \n",
        "                        'wd:' + urlparse(str(ent)).path.split('/')[-1], 'image:' + im['img'][:-4])\n",
        "  if id != None and id.startswith('nm'):\n",
        "    ims = get_image_from_imdb_id(cast=id)\n",
        "    if len(ims) <= 0:\n",
        "      return None, None\n",
        "    im = ims[random.randint(0,len(ims)-1)]\n",
        "    if len(im['movie']) > 0:\n",
        "      i = 0\n",
        "      movie_ent = None\n",
        "      while movie_ent == None and i < len(im['movie']):\n",
        "        id_lit = rdflib.term.Literal(im['movie'][i], datatype=rdflib.term.URIRef('http://www.w3.org/2001/XMLSchema#string'))\n",
        "        for s, p, o in graph.triples((None, imdb_id, id_lit)):\n",
        "          movie_ent = s\n",
        "        i += 1\n",
        "      if movie_ent == None:\n",
        "        return templates['image'][2]%(ent2lbl[ent], 'image:' + im['img'][:-4])\n",
        "      return templates['image'][1]%(ent2lbl[ent].title(), ent2lbl[movie_ent].title(), \n",
        "                                    'wd:' + urlparse(str(movie_ent)).path.split('/')[-1], 'image:' + im['img'][:-4])\n",
        "    else:\n",
        "      return templates['image'][2]%(ent2lbl[ent], 'image:' + im['img'][:-4])\n",
        "  else:\n",
        "    return None\n",
        "\n",
        "def answer_recommend(entities):\n",
        "  graph_emb_obj = []\n",
        "  sentence_emb_obj = []\n",
        "  ans = 'Okay, so '\n",
        "  genres_init = []\n",
        "  names_init = []\n",
        "  for ent in entities:\n",
        "    genre = None\n",
        "    names_init.append(ent2lbl[ent])\n",
        "    for s, p, o in graph.triples((ent, genre_pred, None)):\n",
        "      genre = ent2lbl[o][1:-1]\n",
        "      genres_init.append(genre)\n",
        "\n",
        "  # First using graph embeddings\n",
        "  embeddings = [entities_graph_embeddings[ent2id[ent]] for ent in entities]\n",
        "  lhs = np.mean(embeddings, axis=0)\n",
        "  obj_ids, _ = k_neighbors(lhs, entities_graph_embeddings, k=5)\n",
        "  for i in obj_ids:\n",
        "    obj = id2ent[i]\n",
        "    if ent2lbl[obj] not in names_init:\n",
        "      genre = None\n",
        "      for s, p, o in graph.triples((obj, genre_pred, None)):\n",
        "          genre = ent2lbl[o][1:-1]\n",
        "      for s, p, o in graph.triples((obj, type_pred, None)):\n",
        "          if o == movie_obj and genre in genres_init: # ??????????????????***\n",
        "            graph_emb_obj.append((ent2lbl[obj][1:-1].title(), genre, get_imdb_id(obj), obj))\n",
        "        \n",
        "  if len(graph_emb_obj) == 1:\n",
        "    ans += 'you can check out the movie %s found using the graph embeddings. \\\n",
        "    Its genre is %s.'%(graph_emb_obj[0][0], graph_emb_obj[0][1])\n",
        "    id = graph_emb_obj[0][2]\n",
        "    if id != None and id.startswith('tt'):\n",
        "      ans += templates['recommend'][random.randint(3,5)]%id\n",
        "    if str(graph_emb_obj[0][3]) in plots:\n",
        "      ans += '\\nAlso read the plot here: \\n %s'%plots[graph_emb_obj[0][3]]\n",
        "    return ans, ''\n",
        "  elif len(graph_emb_obj) > 1:\n",
        "    names = []\n",
        "    genres = []\n",
        "    for movie in graph_emb_obj:\n",
        "      name = movie[0]\n",
        "      id = movie[2]\n",
        "      if id != None and id.startswith('tt'):\n",
        "        name += ' (imdb:%s), '%id\n",
        "      \n",
        "      names.append(name)\n",
        "      genres.append(movie[1])\n",
        "    ans = templates['recommend'][0][0]\n",
        "    for name in names:\n",
        "      ans += name\n",
        "    ans = ans[:-2] + templates['recommend'][0][1]\n",
        "    for genre in genres:\n",
        "      ans += genre + ', '\n",
        "    ans = ans[:-2] + templates['recommend'][0][2]\n",
        "    return ans, ''\n",
        "\n",
        "\n",
        "  # Using Sentence Embeddings\n",
        "  lbls = [ent2lbl[ent] for ent in entities]\n",
        "  embeddings = sentence_model.encode(lbls)\n",
        "  lhs = np.mean(embeddings)\n",
        "  obj_ids, _ = k_neighbors(lhs, entities_embeddings, k=10)\n",
        "  for i in obj_ids:\n",
        "    objects = entities_dict[entities_names[i]]\n",
        "    found = False\n",
        "    i = 0\n",
        "    while not found and i < len(objects):\n",
        "      obj = objects[i]\n",
        "      if obj in ent2lbl and ent2lbl[obj] not in names_init:\n",
        "        genre = None\n",
        "        for s, p, o in graph.triples((obj, genre_pred, None)):\n",
        "            genre = ent2lbl[o][1:-1]\n",
        "        for s, p, o in graph.triples((obj, type_pred, None)):\n",
        "            if o == movie_obj and genre in genres_init: # ??????????????????***\n",
        "              sentence_emb_obj.append((ent2lbl[obj].title(), genre, get_imdb_id(obj)))\n",
        "              found = True\n",
        "      i += 1\n",
        "  \n",
        "  if len(sentence_emb_obj) == 1:\n",
        "    ans += templates['recommend'][random.randint(1,2)]%(sentence_emb_obj[0][0], 'Knowledge Base', \n",
        "                                                        sentence_emb_obj[0][1])\n",
        "    id = sentence_emb_obj[0][2]\n",
        "    if id != None and id.startswith('tt'):\n",
        "      ans += templates['recommend'][random.randint(3,5)]%id\n",
        "    # if str(graph_emb_obj[0][3]) in plots:\n",
        "    #  ans += '\\nAlso read the plot here: \\n %s'%plots[graph_emb_obj[0][3]]\n",
        "    return ans, ''\n",
        "\n",
        "  elif len(sentence_emb_obj) > 1:\n",
        "    names = []\n",
        "    genres = []\n",
        "    for movie in graph_emb_obj:\n",
        "      name = movie[0]\n",
        "      id = movie[2]\n",
        "      if id != None and id.startswith('tt'):\n",
        "        name += ' (imdb:%s), '%id\n",
        "      \n",
        "      names.append(name)\n",
        "      genres.append(movie[1])\n",
        "    ans = templates['recommend'][0][0]\n",
        "    for name in names:\n",
        "      ans += name\n",
        "    ans = ans[:-2] + templates['recommend'][0][1]\n",
        "    for genre in genres:\n",
        "      ans += genre + ', '\n",
        "    ans = ans[:-2] + templates['recommend'][0][2]\n",
        "    return ans, ''\n",
        "  return '', ''\n",
        "\n",
        "    \n",
        "# ************************************************EXTRACT****************************************************\n",
        "def extract(q, k=3):\n",
        "  ne, pred = get_named_entities_2(q)\n",
        "  if pred == 'greet':\n",
        "    return templates['greet'][random.randint(1,3)], ''\n",
        "  if len(ne) <= 0:\n",
        "    return query(q), ''\n",
        "\n",
        "  embeddings_ent = sentence_model.encode(ne)\n",
        "  triples = []\n",
        "  embeddings_triples = []\n",
        "  entities = []\n",
        "  \n",
        "  if pred == 'recommend':\n",
        "    for emb in embeddings_ent:\n",
        "      ent_ids = k_neighbors(emb, entities_embeddings, k=10)\n",
        "      ent_id = 0\n",
        "      found = False\n",
        "      while not found and ent_id < len(ent_ids[0]):\n",
        "        i = 0\n",
        "        ents = entities_dict[entities_names[ent_ids[0][ent_id]]]\n",
        "        while not found and i < len(ents):\n",
        "          ent = ents[i]\n",
        "          for s, p, o in graph.triples((ent, type_pred, None)):\n",
        "            if o == movie_obj: # ??????????????????***\n",
        "              entities.append(ent)\n",
        "              found = True\n",
        "          i += 1\n",
        "        ent_id += 1\n",
        "    if len(entities) > 0:\n",
        "      return answer_recommend(entities)\n",
        "    else:\n",
        "      return 'Sorry, I could not find those movies in the KG. ', ''\n",
        "\n",
        "  elif pred == image:\n",
        "    ne_id = 0\n",
        "    found = False\n",
        "    while not found and ne_id < len(embeddings_ent):\n",
        "      emb = embeddings_ent[ne_id]\n",
        "      ent_ids = k_neighbors(emb, entities_embeddings, k=10)\n",
        "      ent_id = 0\n",
        "      while not found and ent_id < len(ent_ids[0]):\n",
        "        i = 0\n",
        "        ents = entities_dict[entities_names[ent_ids[0][ent_id]]]\n",
        "        while not found and i < len(ents):\n",
        "          ent = ents[i]\n",
        "          ans = answer_image(ent)\n",
        "          if ans != None:\n",
        "            return ans, ''\n",
        "          i += 1\n",
        "        ent_id += 1\n",
        "      ne_id += 1\n",
        "    return 'Sorry, could not find the movie or actor you are asking for. ', ''\n",
        "  else:  \n",
        "    entities = [entities_dict[ent_name][0] for ent_name in ne]\n",
        "    for ent in entities:\n",
        "    # check if can use embeddings!\n",
        "      if pred in pred2id and ent in ent2id:\n",
        "        triple = get_objects_embeddings(ent, pred)\n",
        "        if len(triple) > 0 and len(triple[2]) > 0:\n",
        "          embeddings_triples.append(triple)\n",
        "      triple = get_objects(ent, pred)\n",
        "      if len(triple) > 0 and len(triple[2]) > 0:\n",
        "        triples.append(triple)\n",
        "\n",
        "  return triples, embeddings_triples\n",
        "\n",
        "def get_predicate(question):\n",
        "  # check if they just do small talk\n",
        "  emb = sentence_model.encode([question])[0]\n",
        "  n, _ = k_neighbors(emb, np.concatenate([predicates_embeddings, greet_emb.reshape(1,-1)], axis=0), k=1)\n",
        "  if n == len(predicates_embeddings):\n",
        "    return 'greet', question\n",
        "\n",
        "  # first check for look like or looks like\n",
        "  words = ''.join(l for l in question if l in only_letters)\n",
        "  if 'look like' in words:\n",
        "    return image, 'look like'\n",
        "  if 'looks like' in words:\n",
        "    return image, 'looks like'\n",
        "\n",
        "  # then check if recommendation question\n",
        "  if 'recommend' in words:\n",
        "    return 'recommend', 'recommend'\n",
        "  if 'similar' in words:\n",
        "    return 'recommend', 'similar'\n",
        "\n",
        "  else:\n",
        "    # get most probable predicate\n",
        "    q = question.split()\n",
        "    embeddings_pred = sentence_model.encode(q)\n",
        "    pred = (None, 100000, None)\n",
        "    included = 0\n",
        "    for i, emb in enumerate(embeddings_pred):\n",
        "      [p, d] = k_neighbors(emb, predicates_embeddings, 1)\n",
        "      if predicates_names[p[0]] in question:\n",
        "        if len(predicates_names[p[0]]) > included:\n",
        "          included = len(predicates_names[p[0]])\n",
        "          pred = (p[0], d[0], q[i])\n",
        "      elif included <= 0:\n",
        "        if d < pred[1]:\n",
        "          pred = (p[0], d[0], q[i])\n",
        "  predicates_uris = [predicates_dict[predicates_names[pred[0]]][0]]\n",
        "  return predicates_uris[0], pred[2]\n",
        "\n",
        "def get_objects(entity, pred):\n",
        "  print(entity, pred)\n",
        "  objects = []\n",
        "  predicate = graph.objects(pred, RDFS.label)\n",
        "  predicate = str(list(predicate)[0])\n",
        "  for s, p, o in graph.triples((entity, pred, None)):\n",
        "    if o in ent2lbl:\n",
        "      objects.append(ent2lbl[o].title())\n",
        "    elif isinstance(o, Literal):\n",
        "      objects.append(str(o).title())\n",
        "    else:\n",
        "      print('WTF IF HAPPENING IN get_objects ????')\n",
        "  return [ent2lbl[entity].title(), predicate, objects]\n",
        "\n",
        "def get_objects_embeddings(entity, pred):\n",
        "  ent_emb = entities_graph_embeddings[ent2id[entity]]\n",
        "  pred_emb = predicates_graph_embeddings[pred2id[pred]]\n",
        "  lhs = ent_emb + pred_emb\n",
        "  obj_ids, _ = k_neighbors(lhs, entities_graph_embeddings, k=3)\n",
        "  predicate = graph.objects(pred, RDFS.label)\n",
        "  predicate = str(list(predicate)[0])\n",
        "  objects = []\n",
        "  for i in obj_ids:\n",
        "    objects.append(ent2lbl[id2ent[i]].title())\n",
        "  return [ent2lbl[entity].title(), predicate, objects]\n",
        "\n",
        "def get_named_entities(q):\n",
        "  ne = []\n",
        "  if len(ne) == 0:\n",
        "    txt = q.replace(':', ' ')\n",
        "    txt = txt.replace('-', ' ')\n",
        "    if txt[-1] != '?':\n",
        "      txt += '?'\n",
        "    ner_results = nlp(txt)\n",
        "    #' '.join(w[:1].upper() + w[1:] for w in txt.split(' ')))\n",
        "    name = None\n",
        "    txt = txt.lower()\n",
        "    txt = ' '.join(txt.split())\n",
        "    for entry in ner_results:\n",
        "      if entry['entity'][0] == 'B':\n",
        "        if name != None:\n",
        "          ne.append(name.replace(' ##', ''))\n",
        "        name = entry['word']\n",
        "      elif entry['entity'][0] == 'I':\n",
        "        name += ' ' + entry['word']\n",
        "    if name != None:\n",
        "      ne.append(name.replace(' ##', ''))\n",
        "  return ne, txt\n",
        "\n",
        "def get_named_entities_2(q):\n",
        "  ne = []\n",
        "  text = normalize_text(q)\n",
        "  pred, pred_word = get_predicate(text)\n",
        "  if pred == 'greet':\n",
        "    return [], pred\n",
        "  text = text.replace(pred_word, '')\n",
        "  text = normalize_text(text)\n",
        "  indices = list(np.where([word in text for word in entities_names])[0])\n",
        "  indices2 = indices.copy()\n",
        "  for i in indices:\n",
        "    for j in indices:\n",
        "      word = entities_names[i]\n",
        "      sub = entities_names[j]\n",
        "      if i != j and sub in word and j in indices2:\n",
        "        indices2.remove(j)\n",
        "  for i in indices2:\n",
        "    ne.append(entities_names[i])\n",
        "  arr1inds = np.array([len(word) for word in ne]).argsort()\n",
        "  ne = np.array(ne)[arr1inds[::-1]]\n",
        "\n",
        "  if pred == 'recommend':\n",
        "    return ne, pred\n",
        "  return ne[:1], pred\n",
        "\n",
        "# similarity function\n",
        "def k_neighbors(query, embeddings, k=1):\n",
        "  N = embeddings.shape[0]\n",
        "  d = embeddings.shape[1]\n",
        "\n",
        "  # compute distances\n",
        "  distances = np.linalg.norm(embeddings - query, axis = 1)\n",
        "  # select indices of vectors having the lowest distances from the query vector (sorted!)\n",
        "  neighbors = np.argpartition(distances, range(0, k))[:k]\n",
        "  return [neighbors, distances[neighbors]]\n",
        "\n",
        "def matchsubstring(m,n): \n",
        "   seqMatch = SequenceMatcher(None,m,n) \n",
        "   match = seqMatch.find_longest_match(0, len(m), 0, len(n)) \n",
        "   print(m + ', ' + n + ', ' + m[match.a:match.a + match.size])\n",
        "   return m[match.a:match.a + match.size]\n",
        "\n",
        "# word is the words in the entities_names, text is the question\n",
        "def match_words(word, text):\n",
        "  words = word.split()\n",
        "  text_words = text.split()\n",
        "  count = 0\n",
        "  for w in words:\n",
        "    if w not in stop_words and w in text_words:\n",
        "      count += 1\n",
        "    if w not in text_words:\n",
        "      return 0\n",
        "  return count\n"
      ],
      "metadata": {
        "id": "4H96bTyN2u5y"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "i = 7\n",
        "q = 'hello!'\n",
        "answer(q)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "zO44a_JstoxY",
        "outputId": "22af1363-c79e-427f-ce80-2a5ffecc3132"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Hi :)'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "jJjnhOLDmjH2",
        "iPK1SDy_eScM",
        "fxytdEGKfNJn",
        "IM9Ngj2KgIv1",
        "r1LwTcLkjx4S",
        "kzYA3Ryzk3it",
        "L9xzeSuXk5eK"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}